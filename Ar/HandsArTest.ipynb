{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: camera in c:\\users\\dhananjay\\anaconda3\\lib\\site-packages (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install camera\n",
    "import argparse\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import mediapipe as mp\n",
    "from numpy import interp\n",
    "import uuid\n",
    "from typing import Mapping, Tuple\n",
    "from mediapipe.python.solutions import drawing_styles\n",
    "\n",
    "# Minimum number of matches that have to be found\n",
    "# to consider the recognition valid\n",
    "MIN_MATCHES = 15\n",
    "DEFAULT_COLOR = (0, 255, 0)\n",
    "# load the reference surface that will be searched in the video stream\n",
    "dir_name = os.getcwd()\n",
    "np.set_printoptions(formatter={'float': '{: 0.3f}'.format})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OBJ:\n",
    "    def __init__(self, filename, swapyz=False):\n",
    "        \"\"\"Loads a Wavefront OBJ file. \"\"\"\n",
    "        self.vertices = []\n",
    "        self.normals = []\n",
    "        self.texcoords = []\n",
    "        self.faces = []\n",
    "        material = None\n",
    "        for line in open(filename, \"r\"):\n",
    "            if line.startswith('#'): continue\n",
    "            values = line.split()\n",
    "            if not values: continue\n",
    "            if values[0] == 'v':\n",
    "                v = list(map(float, values[1:4]))\n",
    "                if swapyz:\n",
    "                    v = v[0], v[2], v[1]\n",
    "                self.vertices.append(v)\n",
    "            elif values[0] == 'vn':\n",
    "                v = list(map(float, values[1:4]))\n",
    "                if swapyz:\n",
    "                    v = v[0], v[2], v[1]\n",
    "                self.normals.append(v)\n",
    "            elif values[0] == 'vt':\n",
    "                self.texcoords.append(map(float, values[1:3]))\n",
    "            #elif values[0] in ('usemtl', 'usemat'):\n",
    "                #material = values[1]\n",
    "            #elif values[0] == 'mtllib':\n",
    "                #self.mtl = MTL(values[1])\n",
    "            elif values[0] == 'f':\n",
    "                face = []\n",
    "                texcoords = []\n",
    "                norms = []\n",
    "                for v in values[1:]:\n",
    "                    w = v.split('/')\n",
    "                    face.append(int(w[0]))\n",
    "                    if len(w) >= 2 and len(w[1]) > 0:\n",
    "                        texcoords.append(int(w[1]))\n",
    "                    else:\n",
    "                        texcoords.append(0)\n",
    "                    if len(w) >= 3 and len(w[2]) > 0:\n",
    "                        norms.append(int(w[2]))\n",
    "                    else:\n",
    "                        norms.append(0)\n",
    "                #self.faces.append((face, norms, texcoords, material))\n",
    "                self.faces.append((face, norms, texcoords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(img, obj, projection, model, color=False):\n",
    "    \"\"\"\n",
    "    Render a loaded obj model into the current video frame\n",
    "    \"\"\"\n",
    "    vertices = obj.vertices\n",
    "    scale_matrix = np.eye(3) * 3\n",
    "    h, w = model.shape\n",
    "    for face in obj.faces:\n",
    "        face_vertices = face[0]\n",
    "        points = np.array([vertices[vertex - 1] for vertex in face_vertices])\n",
    "        points = np.dot(points, scale_matrix)\n",
    "        # render model in the middle of the reference surface. To do so,\n",
    "        # model points must be displaced\n",
    "        points = np.array([[p[0] + w / 2, p[1] + h / 2, p[2]] for p in points])\n",
    "        dst = cv2.perspectiveTransform(points.reshape(-1, 1, 3), projection)\n",
    "        imgpts = np.int32(dst)\n",
    "        if color is False:\n",
    "            cv2.fillConvexPoly(img, imgpts, DEFAULT_COLOR)\n",
    "        else:\n",
    "            color = hex_to_rgb(face[-1])\n",
    "            color = color[::-1]  # reverse\n",
    "            cv2.fillConvexPoly(img, imgpts, color)\n",
    "\n",
    "    return img\n",
    "\n",
    "def renderObj(img, obj, projection, color=False):\n",
    "    \"\"\"\n",
    "    Render a loaded obj model into the current video frame\n",
    "    \"\"\"\n",
    "    vertices = obj.vertices\n",
    "    scale_matrix = np.eye(3) * 3\n",
    "    h, w = (644,372)\n",
    "    for face in obj.faces:\n",
    "        face_vertices = face[0]\n",
    "        points = np.array([vertices[vertex - 1] for vertex in face_vertices])\n",
    "        points = np.dot(points, scale_matrix)\n",
    "        # render model in the middle of the reference surface. To do so,\n",
    "        # model points must be displaced\n",
    "        points = np.array([[p[0] + w / 2, p[1] + h / 2, p[2]] for p in points])\n",
    "        dst = cv2.perspectiveTransform(points.reshape(-1, 1, 3), projection)\n",
    "        imgpts = np.int32(dst)\n",
    "        if color is False:\n",
    "            cv2.fillConvexPoly(img, imgpts, DEFAULT_COLOR)\n",
    "        else:\n",
    "            color = hex_to_rgb(face[-1])\n",
    "            color = color[::-1]  # reverse\n",
    "            cv2.fillConvexPoly(img, imgpts, color)\n",
    "\n",
    "    return img\n",
    "def projection_matrix(camera_parameters, homography):\n",
    "    \"\"\"\n",
    "    From the camera calibration matrix and the estimated homography\n",
    "    compute the 3D projection matrix\n",
    "    \"\"\"\n",
    "    # Compute rotation along the x and y axis as well as the translation\n",
    "    homography = homography * (-1)\n",
    "    rot_and_transl = np.dot(np.linalg.inv(camera_parameters), homography)\n",
    "    col_1 = rot_and_transl[:, 0]\n",
    "    col_2 = rot_and_transl[:, 1]\n",
    "    col_3 = rot_and_transl[:, 2]\n",
    "    # normalise vectors\n",
    "    l = math.sqrt(np.linalg.norm(col_1, 2) * np.linalg.norm(col_2, 2))\n",
    "    rot_1 = col_1 / l\n",
    "    rot_2 = col_2 / l\n",
    "    translation = col_3 / l\n",
    "    # compute the orthonormal basis\n",
    "    c = rot_1 + rot_2\n",
    "    p = np.cross(rot_1, rot_2)\n",
    "    d = np.cross(c, p)\n",
    "    rot_1 = np.dot(c / np.linalg.norm(c, 2) + d / np.linalg.norm(d, 2), 1 / math.sqrt(2))\n",
    "    rot_2 = np.dot(c / np.linalg.norm(c, 2) - d / np.linalg.norm(d, 2), 1 / math.sqrt(2))\n",
    "    rot_3 = np.cross(rot_1, rot_2)\n",
    "    # finally, compute the 3D projection matrix from the model to the current frame\n",
    "    projection = np.stack((rot_1, rot_2, rot_3, translation)).T\n",
    "    return np.dot(camera_parameters, projection)\n",
    "\n",
    "def hex_to_rgb(hex_color):\n",
    "    \"\"\"\n",
    "    Helper function to convert hex strings to RGB\n",
    "    \"\"\"\n",
    "    hex_color = hex_color.lstrip('#')\n",
    "    h_len = len(hex_color)\n",
    "    return tuple(int(hex_color[i:i + h_len // 3], 16) for i in range(0, h_len, h_len // 3))\n",
    "\n",
    "def init_feature(name):\n",
    "    chunks = name.split('-')\n",
    "    if chunks[0] == 'sift':\n",
    "        detector = cv2.xfeatures2d.SIFT_create()\n",
    "        norm = cv2.NORM_L2\n",
    "    elif chunks[0] == 'surf':\n",
    "        detector = cv2.xfeatures2d.SURF_create(800)\n",
    "        norm = cv2.NORM_L2\n",
    "    elif chunks[0] == 'orb':\n",
    "        detector = cv2.ORB_create(200)\n",
    "        norm = cv2.NORM_HAMMING\n",
    "    elif chunks[0] == 'akaze':\n",
    "        detector = cv2.AKAZE_create()\n",
    "        norm = cv2.NORM_HAMMING\n",
    "    elif chunks[0] == 'brisk':\n",
    "        detector = cv2.BRISK_create()\n",
    "        norm = cv2.NORM_HAMMING\n",
    "    else:\n",
    "        return None, None\n",
    "    if 'flann' in chunks:\n",
    "        if norm == cv2.NORM_L2:\n",
    "            flann_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "        else:\n",
    "            flann_params= dict(algorithm = FLANN_INDEX_LSH,\n",
    "                               table_number = 6, # 12\n",
    "                               key_size = 12,     # 20\n",
    "                               multi_probe_level = 1) #2\n",
    "        matcher = cv2.FlannBasedMatcher(flann_params, {})  # bug : need to pass empty dict (#1329)\n",
    "    else:\n",
    "        matcher = cv2.BFMatcher(norm)\n",
    "    return detector, matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "def getColor(zDist):\n",
    "    c = int(interp(zDist, [0,15], [0,255]))\n",
    "    return (c,c,c)\n",
    "def createLandMarks(hand_landmarks): #-> Mapping[int, mp_drawing.DrawingSpec]:\n",
    "  hand_landmark_style = {}  \n",
    "  for k, v in drawing_styles._HAND_LANDMARK_STYLE.items():\n",
    "    for landmark in k:\n",
    "      c = getColor(abs(hand_landmarks.landmark[landmark].z*100))\n",
    "      r = int(abs(hand_landmarks.landmark[landmark].z*100))\n",
    "      hand_landmark_style[landmark] =   mp_drawing.DrawingSpec(color=c, thickness=drawing_styles._THICKNESS_DOT, circle_radius= r )\n",
    "  return hand_landmark_style        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Load 3D model from OBJ file\n",
    "obj = OBJ(os.path.join(dir_name, 'models/fox.obj'), swapyz=True)  \n",
    "projection = np.float32([[     503.33,   -699.16,    503.33,-130131.43],\n",
    "                         [    1500,    -62.98,     40.02,-391977.22],\n",
    "                         [      0.26,      0.22,      0.94,  -1283.31]])\n",
    "with mp_hands.Hands(static_image_mode=False,min_detection_confidence=0.8, min_tracking_confidence=0.5) as hands: \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # BGR 2 RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Flip on horizontal\n",
    "        image = cv2.flip(image, 1)\n",
    "        \n",
    "        # Set flag\n",
    "        image.flags.writeable = False\n",
    "        \n",
    "        # Detections\n",
    "        results = hands.process(image)\n",
    "        \n",
    "        # Set flag to true\n",
    "        image.flags.writeable = True\n",
    "        \n",
    "        # RGB 2 BGR\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "#         print('Handedness:', results.multi_handedness)\n",
    "\n",
    "        #Get image H ,W\n",
    "        image_height, image_width, _ = image.shape\n",
    "        \n",
    "        # Rendering results\n",
    "        if results.multi_hand_landmarks:\n",
    "            for num, hand_landmarks  in enumerate(results.multi_hand_landmarks):\n",
    "\n",
    "#                 print(\n",
    "#                     f'Index finger tip coordinates: (',\n",
    "#                     f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x * image_width}, '\n",
    "#                     f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y * image_height}) '\n",
    "#                     f'{abs(hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].z*100)})'\n",
    "#                 )\n",
    "                mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS, \n",
    "#                                         mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "#                                         mp_drawing.DrawingSpec(color=(250, 44, 250), thickness=2, circle_radius=2),)\n",
    "                                      createLandMarks(hand_landmarks),\n",
    "                                      mp_drawing_styles.get_default_hand_connections_style())                        \n",
    "                                \n",
    "                lnd1 = hand_landmarks.landmark[4]\n",
    "                lnd2 = hand_landmarks.landmark[0]\n",
    "                lnd3 = hand_landmarks.landmark[17]\n",
    "                lnd4 = hand_landmarks.landmark[8]\n",
    "                lndLst = [[lnd1.x*image_width, lnd1.y* image_height, lnd1.z* image_width],\n",
    "                          [lnd2.x*image_width, lnd2.y* image_height, lnd2.z* image_width],\n",
    "                          [lnd3.x*image_width, lnd3.y* image_height, lnd3.z* image_width], \n",
    "                          [lnd4.x*image_width, lnd4.y* image_height, lnd4.z* image_width],\n",
    "                          [lnd1.x*image_width, lnd1.y* image_height, lnd1.z* image_width]]\n",
    "                              \n",
    "                # if a valid homography matrix was found render cube on model plane\n",
    "\n",
    "                camera_parameters = np.array([[800, 0, 320], [0, 800, 240], [0, 0, 1]])\n",
    "                homography =  np.float32([ [lnd1.x*image_width, lnd2.x* image_width, lnd3.x* image_width],\n",
    "                          [lnd1.y*image_height, lnd2.y* image_height, lnd3.y* image_height],\n",
    "                          [lnd1.z*image_width, lnd2.z* image_width, lnd3.z* image_width]])\n",
    "                projection = projection_matrix(camera_parameters, homography)  \n",
    "\n",
    "                image = renderObj(image, obj, projection, False)\n",
    "\n",
    "        \n",
    "        plot = np.zeros([image_height, image_width, 3], dtype=np.uint8)                \n",
    "        if results.multi_hand_world_landmarks:\n",
    "            for num,hand_world_landmarks in enumerate(results.multi_hand_world_landmarks):                \n",
    "                for idx,landMrk in enumerate(hand_world_landmarks.landmark):\n",
    "                    hand_world_landmarks.landmark[idx].x += 0.5\n",
    "                    hand_world_landmarks.landmark[idx].y += 0.5\n",
    "                mp_drawing.draw_landmarks(plot,hand_world_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "#                 mp_drawing.plot_landmarks(hand_world_landmarks, mp_hands.HAND_CONNECTIONS, azimuth=5)\n",
    "        \n",
    "        cv2.imshow('Plot', plot)\n",
    "        cv2.imshow('Hand Tracking', image)        \n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command line argument parsing\n",
    "# NOT ALL OF THEM ARE SUPPORTED YET\n",
    "# parser = argparse.ArgumentParser(description='Augmented reality application')\n",
    "\n",
    "# parser.add_argument('-r','--rectangle', help = 'draw rectangle delimiting target surface on frame', action = 'store_true')\n",
    "# parser.add_argument('-mk','--model_keypoints', help = 'draw model keypoints', action = 'store_true')\n",
    "# parser.add_argument('-fk','--frame_keypoints', help = 'draw frame keypoints', action = 'store_true')\n",
    "# parser.add_argument('-ma','--matches', help = 'draw matches between keypoints', action = 'store_true')\n",
    "# TODO jgallostraa -> add support for model specification\n",
    "#parser.add_argument('-mo','--model', help = 'Specify model to be projected', action = 'store_true')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args_rectangle = False\n",
    "args_matches = False\n",
    "\n",
    "args_rectangle = True\n",
    "args_matches = True\n",
    "\"\"\"\n",
    "This functions loads the target surface image,\n",
    "\"\"\"\n",
    "homography = None \n",
    "# matrix of camera parameters (made up but works quite well for me) \n",
    "camera_parameters = np.array([[800, 0, 320], [0, 800, 240], [0, 0, 1]])\n",
    "\n",
    "# # create ORB keypoint detector\n",
    "# orb = cv2.ORB_create()\n",
    "# # create BFMatcher object based on hamming distance  \n",
    "# bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "#create detector and mactcher\n",
    "orb, bf = init_feature('brisk')\n",
    "\n",
    "model = cv2.imread(os.path.join(dir_name, 'reference/model.jpg'), 0)\n",
    "# Compute model keypoints and its descriptors\n",
    "kp_model, des_model = orb.detectAndCompute(model, None)\n",
    "# Load 3D model from OBJ file\n",
    "obj = OBJ(os.path.join(dir_name, 'models/fox.obj'), swapyz=True)  \n",
    "# init video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # read the current frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Unable to capture video\")\n",
    "        break \n",
    "    \n",
    "#     kernel = np.array([[0, -1, 0],\n",
    "#                        [-1, 5,-1],\n",
    "#                        [0, -1, 0]])\n",
    "#     frame = cv2.filter2D(src=frame, ddepth=-1, kernel=kernel)\n",
    "    # find and draw the keypoints of the frame\n",
    "    kp_frame, des_frame = orb.detectAndCompute(frame, None)\n",
    "    # match frame descriptors with model descriptors\n",
    "    matches = bf.match(des_model, des_frame)\n",
    "    # sort them in the order of their distance\n",
    "    # the lower the distance, the better the match\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "    # compute Homography if enough matches are found\n",
    "    if len(matches) > MIN_MATCHES:\n",
    "        # differenciate between source points and destination points\n",
    "        src_pts = np.float32([kp_model[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "        dst_pts = np.float32([kp_frame[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "        # compute Homography\n",
    "        homography, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "        if args_rectangle:\n",
    "            # Draw a rectangle that marks the found model in the frame\n",
    "            h, w = model.shape\n",
    "            pts = np.float32([[0, 0], [0, h - 1], [w - 1, h - 1], [w - 1, 0]]).reshape(-1, 1, 2)\n",
    "            # project corners into frame\n",
    "            dst = cv2.perspectiveTransform(pts, homography)\n",
    "            # connect them with lines  \n",
    "            frame = cv2.polylines(frame, [np.int32(dst)], True, 255, 3, cv2.LINE_AA)  \n",
    "        # if a valid homography matrix was found render cube on model plane\n",
    "        if homography is not None:\n",
    "            try:\n",
    "                # obtain 3D projection matrix from homography matrix and camera parameters\n",
    "                projection = projection_matrix(camera_parameters, homography)  \n",
    "                # project cube or model\n",
    "                print(\"projection : \" ,np.array_str(projection, precision=2, suppress_small=True))\n",
    "                print(\"camera_parameters : \" ,np.array_str(camera_parameters, precision=2, suppress_small=True))\n",
    "                print(\"homography : \" ,np.array_str(homography, precision=2, suppress_small=True))                \n",
    "                frame = render(frame, obj, projection, model, False)\n",
    "                print(frame.shape)\n",
    "                #frame = render(frame, model, projection)\n",
    "            except:\n",
    "                pass\n",
    "        # draw first 10 matches.\n",
    "        if args_matches:\n",
    "            frame = cv2.drawMatches(model, kp_model, frame, kp_frame, matches[:10], 0, flags=2)\n",
    "        # show result\n",
    "        cv2.imshow('frame', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        print(\"Not enough matches found - %d/%d\" % (len(matches), MIN_MATCHES))\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
